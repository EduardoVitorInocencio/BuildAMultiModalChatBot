{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic API key exists and begings with sk-ant-\n",
      "OpenAI API key exists and begings with sk-proj-\n",
      "Groq API key exists and begings with gsk_lVJs\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API key exists and begings with {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"No Anthropic API key found. Please set it in a .env file.\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API key exists and begings with {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"No OpenAI API key found. Please set it in a .env file.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API key exists and begings with {groq_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"No Groq API key found. Please set it in a .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Anthropic API\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "# groq = ChatGroq(api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "it turns out that LLMs don't do a great job of telling jokes! Let's compare a few models. Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "**Typycally we'll pass to the API:**\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is tipically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes. Ans\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists. in Portguese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui est√° uma piada para cientistas de dados:\n",
      "\n",
      "Por que o cientista de dados ficou desempregado?\n",
      "\n",
      "Porque ele n√£o conseguia fazer uma boa *previs√£o* do mercado de trabalho! \n",
      "\n",
      "*ba dum tss* ü•Å\n",
      "\n",
      "Alternativa:\n",
      "\n",
      "Qual √© o lanche preferido do cientista de dados?\n",
      "\n",
      "*Cookie*s... especialmente os de terceiros! üç™\n",
      "\n",
      "(Trocadilho com \"third-party cookies\" - cookies de terceiros, um termo comum em dados web)\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you can evaluate the following criteria:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Based**: Is the problem primarily text-based, involving natural language processing (NLP) tasks such as understanding, generating, or summarizing text?\n",
       "- **Complexity**: Does the problem require understanding context, sentiment, or nuanced language?\n",
       "\n",
       "## 2. Data Availability\n",
       "- **Quality of Data**: Do you have access to high-quality, relevant textual data for training or fine-tuning the LLM?\n",
       "- **Quantity of Data**: Is there enough data available to train or fine-tune an LLM effectively?\n",
       "\n",
       "## 3. Business Objectives\n",
       "- **Clear Goals**: Are the objectives specific and measurable? For example, improving customer support response times or generating marketing content.\n",
       "- **Value Addition**: Will using an LLM provide significant value, such as increased efficiency, cost savings, or enhanced customer experience?\n",
       "\n",
       "## 4. Technical Feasibility\n",
       "- **Infrastructure**: Do you have the necessary computational resources to run an LLM, or can you leverage cloud-based solutions?\n",
       "- **Integration**: Can the LLM be integrated into your existing systems and workflows without excessive disruption?\n",
       "\n",
       "## 5. User Interaction\n",
       "- **User-Centric**: Will users interact with the solution in a meaningful way, such as through chatbots, virtual assistants, or content generation tools?\n",
       "- **Feedback Loop**: Is there a mechanism for users to provide feedback to improve the model‚Äôs performance over time?\n",
       "\n",
       "## 6. Ethical Considerations\n",
       "- **Bias and Fairness**: Are there concerns about bias in the data that could lead to unfair or unethical outcomes?\n",
       "- **Transparency**: Is it important for the solution to be interpretable and explainable to users and stakeholders?\n",
       "\n",
       "## 7. Regulatory Compliance\n",
       "- **Data Privacy**: Are there legal and regulatory considerations regarding data usage, especially in sensitive industries like healthcare or finance?\n",
       "- **Compliance**: Does the solution align with industry regulations and standards?\n",
       "\n",
       "## Conclusion\n",
       "If the business problem fits well within these criteria, it is likely suitable for an LLM solution. Conducting a thorough assessment will help ensure that you make an informed decision about leveraging LLM technology in your business context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, a simple \"hi\"? How original. Can\\'t we aim for a little more creativity here?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        # print(gpt)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        # print(claude_message)\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you doing today?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another \"hi.\" How original. What do you want?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
